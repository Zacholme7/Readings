------------------------
| 3: Memory Management | 
------------------------
- "programs expand to fill memory available to them"

------------------------------------
---- 3.1: No Memory Abstraction ----
------------------------------------
- 1980s and before every program just say entire physical memory

---- Running Multiple Programs without a memory abstraction ----
---------------------------------------------------------------- 
- have to save memory to file and bring in other program

---- Summary ----
-----------------
- Early computers didnt have any abstraction over memory and just ran
all processes in the same physical memory space.

---- Memory Notes ----
----------------------
- None




--------------------------------------------------
---- 3.2: A Memory Abstraction: Address space ----
--------------------------------------------------

---- The Notion of an address space ----
----------------------------------------
- to allow multiple application in memory, protection and relocation have to be solved
- address space: set of addresses that a process can use to address memory
  - created kind of abstract memory for programs to use
- each process has its own address space
- simple solution to address space
  - dynamic relocation
  - map each processes address space onto different part of physical memory
  - cpu has base and limit reg
  - base reg has addr of start of program, limit loaded with length of program
  - add addr loc to base reg and make sure within limit
  - disadvantage is additional comparison for each memory reference

---- Swapping ----
------------------
- total amount of RAM needed often much more than what can fit in mem
- swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- creates holes in memory but can use memory compaction to fill in
- allocate initial program and room to grow

---- Managing Free Memory ----
------------------------------
- can use bitmaps and free lists to keep track of memory usage
1) bitmap: memory divded into allocation units in size from a few words to several kilobytes
  - each allocation unit has bit, 0 for free and 1 for occupied
  - searching bit map is slow
2) linked list: linked list of allocated and free memory segements
  - segement either process or empty hole between two processes
  - several algorithms for allocation. first fit, next fit, best fit, quick fit, ...

---- Summary ----
-----------------
- The most basic memory abstration is the concept of an address space which is just
a set of addresses that a process can use to address its memory. Each process has its own address
space, so we need to have some way to fit all of the address spaces in memory when there are multiple processes
running. There are various algorithms to achieve this but none of them presented here are optimal, but the
concept of swapping is important in the final solution

---- Memory Notes ----
----------------------
- address space: set of addresses that a process can use to address memory
- addr space swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- mem mgmt w/ bitmap: memory divded into allocation units in size from a few words to several kilobytes, each allocation unit 
has bit, 0 for free and 1 for occupied, searching bit map is slow
- mem mgmt w/ linked list: linked list of allocated and free memory segements, segement either process or 
empty hole between two processes, several algorithms for allocation. first fit, next fit, best fit, quick fit, ...




-----------------------------
---- 3.3: Virtual Memory ----
-----------------------------
- there is a need to run programs that are too large to fit in memory
- virtual memory: program has its own addr space that is broken into pages
  - page: a contiguous range of addresses mapped onto physical memory
  - not all pages have to be present

---- Paging ----
----------------
- virtual addresses: program generated addresses that form the virtual address space
  - go to the mmu
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- present/absent bit keeps track of which pages are physically present in memory
- if MMU sees page is unmapped, page fault occurs
  - writes unused page to disk and brings in needed page
- page number used as index into page table, other bits used as offset
- process
1) program issue adress to access byte in page
2) mmu checks page table to see if page is mapped to physical frame
  - if mapped, retrieve data
3) if not mapped, issue page fault bc it needs to load the page into memory
4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped
6) restart the instruction

---- Page Tables ----
---------------------
- virtual address split into virtual page number (high order bits) and offset (low order bits)
- virtual page number used as index into page table to find entry for that virtual page
  - if found, page frame number attatched to offset to form physical address for memory
- purpose of page table it so map virtual pages onto page frames
- structure of a page table entry
  - varies but general form is consistent
  - page frame number
  - present/absent bit: indicates wherther entry is valid and can be used
  - protection: tells what kind of actions are permitted
  - supervisor: indicates whether page is accessible to only privilged code
  - modified: tracks if page was modified, must be written back if so
  - referenced: keeps track of references, needed to decided which to exict
  - cache disable: can turn off caching
- page table only holds info the hardware needs to tranlsate virtual addr into physical

---- Speeding up Paging ----
------------------------------
- 2 issues must be addresses
  1) mapping from virtual -> physical must be fast
  2) even if virtual addr space very large, page table must not be large
- mapping has to be done on every memory reference
- most programs tend to make a large number of references to a small number of pages
  - only fraction of pte are heavily read, rest barely used
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table
  - usually inside the mmu
- when address presented to mmu for tranlsation
1) hardward checks to see if virtual page number in TBL (done in parallel via hardware)
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory
3) if no match found, does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 

---- Page Tables for Large Memories ----
----------------------------------------
1) Multilevel page tables
- multiple levels of page tables
- addr broken into indexes, pt1, pt2, ...
- can keep nesting levels
2) inverted page tables
- one entry per page frame in real memory rather than one entry per page of virtual addr space
- entry keeps track of which (process, virtual page) is located in page frame
- downside is that it must search entire table
  - TLB and hashing can speed up

---- Summary ----
-----------------

---- Memory Notes ----
- virtual memory: program has its own addr space that is broken into pages
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual addresses: program generated addresses that form the virtual address space
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- what happens if mmu tries to access unmapped page? page fault, trap in os, write unused page to disk and bring
in needed page
- process of an address fetch w/ virtual memory: 1) program issue adress to access byte in page, 2) mmu checks page table to see if page is mapped to physical frame, 
if mapped, retrieve data, 3) if not mapped, issue page fault bc it needs to load the page into memory 4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped 6) restart the instruction
- virtual add split into? virtual page number and offset
- what is virtual page number used for? index into page table, if page found, page frame attatched to offset to form physical addr
- purpose of page table? map virtual pages onto page frames
- what is the purpose of virtual memory? create an abstraction over the address space, which is an abstraction of physical memory
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table, inside the mmu
- process when address presented to mmu for tranlsation: 1) hardward checks to see if virtual page number in TBL (done in parallel via hardware), 
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory, 3) if no match found, 
mmu does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 
- multilevel page table: hierarchical page table structure, poritions of virtual addr used to index into various levels of page table, used for handling
large address spaces while minimizing the memory required for the page table itself
- inverted page table: optimize amount of memory used by page table itself, single table for all physical memory frames, each entry corresponds to physical frame and
stores info about which virtual frame it holds




------------------------------------------
---- 3.4: Page Replacement Algorithms ----
------------------------------------------

---- The Optimal Page Replacement Algorithm ----
------------------------------------------------
- easy to describe impossible to implement
- at moment of page fault, evict page that will be used farthest away
- can do in two passes, not one

---- The Not Recently Used Page Replacement Algorithm ----
----------------------------------------------------------
- two status bits, R and M
  R: set whenever page is referenced (read or written)
  M: set whenever the page was modified
- must be updated on every memory reference
- when process started up, both page bits for all pages set to 0 by os
- periodically clearly R bit to indicate not referenced
- on page fault, pages divided into categories
Class 0: not referenced, not modified
Class 1: not referenced, modified
Class 2: referenced, not modified
Class 3: referenced, modified
- remove a page at random from lowest numbered nonempty class

---- The First in First out Page Replacement Algorithm ----
-----------------------------------------------------------
- os maintains list of all pages in memory
  - most recent arrival at tail
  - least recent arrival at head
- on page fault, remove from head and add to tail

---- The Second Change Page Replacement Algorithm ----
------------------------------------------------------
- modification to FIFO that insepct R bit of oldest page
- if 0, evicted and replaced
- if 1, clear bit and put page onto end of list
  - keep looking for page with 0 R to evict

---- The Clock Page Replacement Algorithm ----
----------------------------------------------
- keep pages in circular list in form of clock
- same algo as second change occurs
- page pointed to by hand inspected
- if R is 0, evicted and new page inserted
- if R is 1, clear R and advance hand

---- The Least Recently Used Page Algorithm ----
------------------------------------------------
- observation that page that have been used in last few instructions will be used again
- observation that pages that have not been used in a while will continue not to be used
- maintain list that must be updated on every memory reference
  - this is expensive and requires hardware support to be feasible

---- Simulating LRU in Software ----
------------------------------------
- software counter with each page 
- on interrupts, os scan all page and R bit added to counter for page
- page with lowest counter chosen for replacement
- modifications to make it work better

---- The Working Set Page Replacment Algorithm ----
---------------------------------------------------
- processes started up with no pages and brought in as needed
- demand paging: pages are loaded only on demand, not in advanced
- most processes exibit locality so only using a small number of pages
- set of pages processes currently using is its working set
- thrashing: program causing page faults every few instructions
- working set model: keep track of processes working set and prepage them in
- evict pages not in the working set (k most recent pages)
- can track age of page in time

---- The WSClock Page Replacement Algorithm ----
------------------------------------------------
- widely used in practice
- circular list of page frames
- each entry contains time of last use, R bit, M bit
- if R bit is zero and time of last use is greater than some threshold, evict
  - if page dirty, write to storage scheduled but hand advanced to look for another page

---- Summary of Page Replacement Algorithms ----
------------------------------------------------
- Optimal: Not implementable, but useful as a benchmark
- NRU (Not Recently Used): Very crude approximation of LRU
- FIFO (First-In, First-Out): Might throw out important pages
- Second chance: Big improvement over FIFO
- Clock: Realistic
- LRU (Least Recently Used): Excellent, but difficult to implement exactly
- NFU (Not Frequently Used): Fairly crude approximation to LRU
- Aging: Efficient algorithm that approximates LRU well
- Working set: Somewhat expensive to implement
- WSClock: Good efficient algorithm

---- Summary ----
-----------------
-

---- Memory Notes ----
----------------------
- 




-----------------------------------------------
---- 3.5: Design Issues For Paging Systems ----
-----------------------------------------------

---- Local versus Global Allocation Policies ----
-------------------------------------------------
- how should memory be allocated among competing runnable processes???
- Local algorithm: allocating every process fixed fraction of memory
- Global algorithm: allocating memory dynamically among runnable processes
- can allocate pages in proportion to process size, with some minimum
- PFF (Page fault frequency): tells when to increase or decrease a process' page allocation
  - fault rate decreases as more pages are assigned
- trade offs between local and global, comes down to amount of page faults

---- Load Control ----
----------------------
- working set of all processes exceed memory, but one process may need more and none need less
  - no way to give more memory
- OS has process called OOM (out of memory killer) that becomes active when system low on memory
  - score processes based on "badness" and kill ones with highest scores
- can also swap process to nonvolitaile storage to free mem, compress, and compact

---- Cleaning Policy ----
-------------------------
- paging daemon: sleeps most of time but periodically awakened to insepct state of memory
  - if too few page frames are free, selets pages to evict
- makes sure free frames are clean and do not need to be written when required
- two handed clock: one hand writing dirty pages and other for replacement

---- Page Size ----
-------------------
- on average, half of final page will be empty
  - extra wasted space called internal fragementation
  - n segements, size of p bytes, n * p / 2 wasted
  - reason for smaller page size
- small pages mean need many pages and larger page table
  - same time read/write to disk as large page
- small pages use up valuable space in tlb

---- Separate Instruction and Data Spaces ----
----------------------------------------------
- most computers have addr space that holds program and data
  - if addr space is not large enough, problems arise
- can seperate program text (I space) and data (D space)
  - although, each need own page table and mapping
  - used for L1 cache today

---- Shared Pages ----
----------------------
- sharing pages is straightfoward with I&D pages
  - two+ processes shared same I space and use different D space
- harder with single addr space
  - if two processes sharing, evicting one process & its pages will cause other to page fault
  - if one process terminates, have to figure out which pages can be cleaned up

---- Shared Libraries ----
--------------------------















