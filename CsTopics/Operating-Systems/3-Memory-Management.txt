------------------------
| 3: Memory Management | 
------------------------
- "programs expand to fill memory available to them"

------------------------------------
---- 3.1: No Memory Abstraction ----
------------------------------------
- 1980s and before every program just say entire physical memory

---- Running Multiple Programs without a memory abstraction ----
---------------------------------------------------------------- 
- have to save memory to file and bring in other program

---- Summary ----
-----------------
- Early computers didnt have any abstraction over memory and just ran
all processes in the same physical memory space.

---- Memory Notes ----
----------------------
- None




--------------------------------------------------
---- 3.2: A Memory Abstraction: Address space ----
--------------------------------------------------

---- The Notion of an address space ----
----------------------------------------
- to allow multiple application in memory, protection and relocation have to be solved
- address space: set of addresses that a process can use to address memory
  - created kind of abstract memory for programs to use
- each process has its own address space
- simple solution to address space
  - dynamic relocation
  - map each processes address space onto different part of physical memory
  - cpu has base and limit reg
  - base reg has addr of start of program, limit loaded with length of program
  - add addr loc to base reg and make sure within limit
  - disadvantage is additional comparison for each memory reference

---- Swapping ----
------------------
- total amount of RAM needed often much more than what can fit in mem
- swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- creates holes in memory but can use memory compaction to fill in
- allocate initial program and room to grow

---- Managing Free Memory ----
------------------------------
- can use bitmaps and free lists to keep track of memory usage
1) bitmap: memory divded into allocation units in size from a few words to several kilobytes
  - each allocation unit has bit, 0 for free and 1 for occupied
  - searching bit map is slow
2) linked list: linked list of allocated and free memory segements
  - segement either process or empty hole between two processes
  - several algorithms for allocation. first fit, next fit, best fit, quick fit, ...

---- Summary ----
-----------------
- The most basic memory abstration is the concept of an address space which is just
a set of addresses that a process can use to address its memory. Each process has its own address
space, so we need to have some way to fit all of the address spaces in memory when there are multiple processes
running. There are various algorithms to achieve this but none of them presented here are optimal, but the
concept of swapping is important in the final solution

---- Memory Notes ----
----------------------
- address space: set of addresses that a process can use to address memory
- addr space swapping: bringing in each process, running it for a while, then moving it to nonvolatile storage
- mem mgmt w/ bitmap: memory divded into allocation units in size from a few words to several kilobytes, each allocation unit 
has bit, 0 for free and 1 for occupied, searching bit map is slow
- mem mgmt w/ linked list: linked list of allocated and free memory segements, segement either process or 
empty hole between two processes, several algorithms for allocation. first fit, next fit, best fit, quick fit, ...




-----------------------------
---- 3.3: Virtual Memory ----
-----------------------------
- there is a need to run programs that are too large to fit in memory
- virtual memory: program has its own addr space that is broken into pages
  - page: a contiguous range of addresses mapped onto physical memory
  - not all pages have to be present

---- Paging ----
----------------
- virtual addresses: program generated addresses that form the virtual address space
  - go to the mmu
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- present/absent bit keeps track of which pages are physically present in memory
- if MMU sees page is unmapped, page fault occurs
  - writes unused page to disk and brings in needed page
- page number used as index into page table, other bits used as offset
- process
1) program issue adress to access byte in page
2) mmu checks page table to see if page is mapped to physical frame
  - if mapped, retrieve data
3) if not mapped, issue page fault bc it needs to load the page into memory
4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped
6) restart the instruction

---- Page Tables ----
---------------------
- virtual address split into virtual page number (high order bits) and offset (low order bits)
- virtual page number used as index into page table to find entry for that virtual page
  - if found, page frame number attatched to offset to form physical address for memory
- purpose of page table it so map virtual pages onto page frames
- structure of a page table entry
  - varies but general form is consistent
  - page frame number
  - present/absent bit: indicates wherther entry is valid and can be used
  - protection: tells what kind of actions are permitted
  - supervisor: indicates whether page is accessible to only privilged code
  - modified: tracks if page was modified, must be written back if so
  - referenced: keeps track of references, needed to decided which to exict
  - cache disable: can turn off caching
- page table only holds info the hardware needs to tranlsate virtual addr into physical

---- Speeding up Paging ----
------------------------------
- 2 issues must be addresses
  1) mapping from virtual -> physical must be fast
  2) even if virtual addr space very large, page table must not be large
- mapping has to be done on every memory reference
- most programs tend to make a large number of references to a small number of pages
  - only fraction of pte are heavily read, rest barely used
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table
  - usually inside the mmu
- when address presented to mmu for tranlsation
1) hardward checks to see if virtual page number in TBL (done in parallel via hardware)
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory
3) if no match found, does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 

---- Page Tables for Large Memories ----
----------------------------------------
1) Multilevel page tables
- multiple levels of page tables
- addr broken into indexes, pt1, pt2, ...
- can keep nesting levels
2) inverted page tables
- one entry per page frame in real memory rather than one entry per page of virtual addr space
- entry keeps track of which (process, virtual page) is located in page frame
- downside is that it must search entire table
  - TLB and hashing can speed up

---- Summary ----
-----------------
- Virtual memory is an abstraction over memory that provides each process with the illusion of having nearly unlimited access
to memory in a contiguous sequence. The virtual address spaces is broken up onto multiple pages, which each contain a section 
of the virtual address space. When a process tries to access an address in its virtual address space, that address is send to the 
memory management unit to be transformed into a physical address that can be used to access the value at that location. If the current
virtual address's page is not in memory, a page currently in memory needs to be evicted and the new one brought in. Eviction is handled 
thorugh various algorithms. Paging can be slow. You must map from a virutal address to a physical address and then have to find the associated 
page very quyickly. The translation lookaside buffer is a hardware cache that caches mappings so you can avoid a page table lookup. This helps 
speed up paging. As for the structure of page tables, the two main approaches are multilevel page tables and inverted pages tables

---- Memory Notes ----
- virtual memory: program has its own addr space that is broken into pages
- memory management unit: maps virtual addresses onto physical memory addresses
- virtual addresses: program generated addresses that form the virtual address space
- virtual address space: all virtual addresses divided into pages
- pages: fixed size units of virtual addresses
- page frame: corresponding physical memory to a page
- what happens if mmu tries to access unmapped page? page fault, trap in os, write unused page to disk and bring
in needed page
- process of an address fetch w/ virtual memory: 1) program issue adress to access byte in page, 2) mmu checks page table to see if page is mapped to physical frame, 
if mapped, retrieve data, 3) if not mapped, issue page fault bc it needs to load the page into memory 4) select little used page to write to disk, then fetch needed page
5) set old page to unmapped and new page to mapped 6) restart the instruction
- virtual add split into? virtual page number and offset
- what is virtual page number used for? index into page table, if page found, page frame attatched to offset to form physical addr
- purpose of page table? map virtual pages onto page frames
- what is the purpose of virtual memory? create an abstraction over the address space, which is an abstraction of physical memory
- Translation Lookaside Buffer: hardware device for mapping virtual addresses to physical addresses without going through page table, inside the mmu
- process when address presented to mmu for tranlsation: 1) hardward checks to see if virtual page number in TBL (done in parallel via hardware), 
2) if valid match found and access does not violate protection, page frame taken from TLB w/o going to page table in memory, 3) if no match found, 
mmu does ordinary page table lookup and evicts page from TLB
- soft miss: page referenced is not in the TLB, but is in memory, 10 - 20 machine instrs to handle
- hard miss: page itself is not in memory, can be a million times slower than soft miss
- segmentation fault: program accessed invalid address 
- multilevel page table: hierarchical page table structure, poritions of virtual addr used to index into various levels of page table, used for handling
large address spaces while minimizing the memory required for the page table itself
- inverted page table: optimize amount of memory used by page table itself, single table for all physical memory frames, each entry corresponds to physical frame and
stores info about which virtual frame it holds




------------------------------------------
---- 3.4: Page Replacement Algorithms ----
------------------------------------------

---- The Optimal Page Replacement Algorithm ----
------------------------------------------------
- easy to describe impossible to implement
- at moment of page fault, evict page that will be used farthest away
- can do in two passes, not one

---- The Not Recently Used Page Replacement Algorithm ----
----------------------------------------------------------
- two status bits, R and M
  R: set whenever page is referenced (read or written)
  M: set whenever the page was modified
- must be updated on every memory reference
- when process started up, both page bits for all pages set to 0 by os
- periodically clearly R bit to indicate not referenced
- on page fault, pages divided into categories
Class 0: not referenced, not modified
Class 1: not referenced, modified
Class 2: referenced, not modified
Class 3: referenced, modified
- remove a page at random from lowest numbered nonempty class

---- The First in First out Page Replacement Algorithm ----
-----------------------------------------------------------
- os maintains list of all pages in memory
  - most recent arrival at tail
  - least recent arrival at head
- on page fault, remove from head and add to tail

---- The Second Chance Page Replacement Algorithm ----
------------------------------------------------------
- modification to FIFO that insepct R bit of oldest page
- if 0, evicted and replaced
- if 1, clear bit and put page onto end of list
  - keep looking for page with 0 R to evict


---- The Clock Page Replacement Algorithm ----
----------------------------------------------
- keep pages in circular list in form of clock
- same algo as second change occurs
- page pointed to by hand inspected
- if R is 0, evicted and new page inserted
- if R is 1, clear R and advance hand


---- The Least Recently Used Page Algorithm ----
------------------------------------------------
- observation that page that have been used in last few instructions will be used again
- observation that pages that have not been used in a while will continue not to be used
- maintain list that must be updated on every memory reference
  - this is expensive and requires hardware support to be feasible


---- Simulating LRU in Software ----
------------------------------------
- software counter with each page 
- on interrupts, os scan all page and R bit added to counter for page
- page with lowest counter chosen for replacement
- modifications to make it work better


---- The Working Set Page Replacment Algorithm ----
---------------------------------------------------
- processes started up with no pages and brought in as needed
- demand paging: pages are loaded only on demand, not in advanced
- most processes exibit locality so only using a small number of pages
- set of pages processes currently using is its working set
- thrashing: program causing page faults every few instructions
- working set model: keep track of processes working set and prepage them in
- evict pages not in the working set (k most recent pages)
- can track age of page in time

---- The WSClock Page Replacement Algorithm ----
------------------------------------------------
- widely used in practice
- circular list of page frames
- each entry contains time of last use, R bit, M bit
- if R bit is zero and time of last use is greater than some threshold, evict
  - if page dirty, write to storage scheduled but hand advanced to look for another page

---- Summary of Page Replacement Algorithms ----
------------------------------------------------
- Optimal: Not implementable, but useful as a benchmark
- NRU (Not Recently Used): Very crude approximation of LRU
- FIFO (First-In, First-Out): Might throw out important pages
- Second chance: Big improvement over FIFO
- Clock: Realistic
- LRU (Least Recently Used): Excellent, but difficult to implement exactly
- NFU (Not Frequently Used): Fairly crude approximation to LRU
- Aging: Efficient algorithm that approximates LRU well
- Working set: Somewhat expensive to implement
- WSClock: Good efficient algorithm

---- Summary ----
-----------------
- When a page fault occurs, the new page must be brought into memory. It is often the case that memory is already full of pages from other proess. When this
occurs we need to evict a page from memory to include the newly accessed page. There are various algorithms proposed for this which each have their tradeoffs. 
Ideally, we want an algorithm that will replace a page that will be used farthest in the future but this is not possible to implement in one pass. We cant reach
optimal, but there are efficient algorithms that perform this job reasonably sufficiently. 

---- Memory Notes ----
----------------------
- What is the optimal, but impossible to implement page replacement algorithm? At moment of page fault, you evict the page that will be used farthest away in the future, not implementable
- Not Recently used page replacement: R bit -> set whenever page references, M bit -> set whenever page modified. Pages divided into categories. Class 0: not references or modified.
Class 1: not referenced, modified. Class 2: references, not modified. Class 3: referenced, modified. Remove page at random from lowest numbered nonempty class, crude approx of LRU
- First in First out Page Replacement: Linked list of pages, most recent arrival at tail and oldest at head. on fault, remove from head and add to tail, might throw out important pages
- Second Chance Page Replacment: Modified FIFO, linked list like fifo but on page fault inspect R bit of oldest page (head). If 0, evict, else move it to end of list and keep looking 
- Clock Page Replacement: Keep pages in circular list. Same algorithm as second chance. If R is 0, evice and insert new page, if R is one, clear R and advance hand, realistic
- Least Recently used page replacement: page not used recently will likely not be used again soon, maintain list of references and remove least recently used page, Excellent, difficult to impl
- Working set page replacement: set of pages process using is working set, keep track of process working set and evict pages not in working set, expensive to impl
- Workign set clock page replacement: Circular list of pages, R bit = 0 and time of last use greater than some threadshold, evict, good and efficient
- demand paging: pages are loaded only on demand




-----------------------------------------------
---- 3.5: Design Issues For Paging Systems ----
-----------------------------------------------

---- Local versus Global Allocation Policies ----
-------------------------------------------------
- how should memory be allocated among competing runnable processes???
- Local algorithm: allocating every process fixed fraction of memory
- Global algorithm: allocating memory dynamically among runnable processes
- can allocate pages in proportion to process size, with some minimum
- PFF (Page fault frequency): tells when to increase or decrease a process' page allocation
  - fault rate decreases as more pages are assigned
- trade offs between local and global, comes down to amount of page faults


---- Load Control ----
----------------------
- working set of all processes exceed memory, but one process may need more and none need less
  - no way to give more memory
- OS has process called OOM (out of memory killer) that becomes active when system low on memory
  - score processes based on "badness" and kill ones with highest scores
- can also swap process to nonvolitaile storage to free mem, compress, and compact

---- Cleaning Policy ----
-------------------------
- paging daemon: sleeps most of time but periodically awakened to insepct state of memory
  - if too few page frames are free, selets pages to evict
- makes sure free frames are clean and do not need to be written when required
- two handed clock: one hand writing dirty pages and other for replacement


---- Page Size ----
-------------------
- on average, half of final page will be empty
  - extra wasted space called internal fragementation
  - n segements, size of p bytes, n * p / 2 wasted
  - reason for smaller page size
- small pages mean need many pages and larger page table
  - same time read/write to disk as large page
- small pages use up valuable space in tlb

---- Separate Instruction and Data Spaces ----
----------------------------------------------
- most computers have addr space that holds program and data
  - if addr space is not large enough, problems arise
- can seperate program text (I space) and data (D space)
  - although, each need own page table and mapping
  - used for L1 cache today

---- Shared Pages ----
----------------------
- sharing pages is straightfoward with I&D pages
  - two+ processes shared same I space and use different D space
- harder with single addr space
  - if two processes sharing, evicting one process & its pages will cause other to page fault
  - if one process terminates, have to figure out which pages can be cleaned up

---- Shared Libraries ----
--------------------------
- in modern systems, many systems have to share large libraries
- when program linked with shared library, small stud routine that binds to called function at runtime
included
  - functions paged in when needed

---- Mapped Files ----
----------------------
- process can issue syscall to map file onto portion of its virtual addr space
- alternate model to I/O
  - file accessed as big character array in memory


---- Summary ----
-----------------
- There are many tradeoffs to consider when you are implementing paging algorithms. One choice can have results that trickle down to other
parts of the systems and have impacts of performance. 
 
---- Memory Notes ----
----------------------
- What are design issues for paging systems? 1) local vs global allocation: local -> allocate every process fixed fraction of memory. Global: allocating memory dynamically among runnable processes.
2) Load Control: working set of all processes may exceed memory and a process may need more. OOM (out of memory killer) rank processes and kill ones, can also compact and compress. 3) cleaning policy:
paging daemon sleeps most of time but periodically awakened to select pages to evict or write to memory. 4) page size: on average half of final pages will be empty. Dont want page size too small so we 
do a ton of swaps but also dont want too bit so we waste too much memory. 5) Seperate instruction and data spaces: can sepearate to save/share space but need own page table. 6) Shared pages: easy with
seperate I&D spaces but if not seperate have to manage how to share when one process killed. 





------------------------------------
---- 3.6: Implementation Issues ----
------------------------------------

---- Operating System Involvement with Paging ----
--------------------------------------------------
- has to decide how large program & data will be initially and create page table
- space needs to be allocated
- page table needs to be ready to go
- MMU rest and TLB flushed
- clean up page table
- various other duties

---- Page Fault Handling ----
-----------------------------
1) hardware trap to kernel, pc saved on stack and cpu information saved
2) ISR started to save regs and other volatile info, then call page fault handler
3) os try to discover which virtual page needed
4) once have virtual addr which casued fault, validate addr and look for free page frame
5) if page frame dirty, write to nonvolatile
6) when page frame clean, bring in needed page
7) update page table when page arrives
8) instruction backed up to state it had when it began, pc reset too
9) process is rescheduled
10) registers and other state information restored, return to user space

---- Instruction Backup ----
----------------------------
- must determine where first byte of instruction is 

---- Locking Pages in Memory ----
---------------------------------
- I/O must still occur
- chance that page containing I/O buffer will be chosen and removed from memory
- can lock/pin a page to make sure that it is not removed

---- Backing Store ----
-----------------------
- allocating space on nonvolatile storage uses swap parition
- new processes assigned chunks of swap parition
  - associated w/ it is the address of its nonvolatile storage area
- other algorithms for this

---- Summary ----
-----------------
- Along with paging comes various implementation issues. Some of these issues include configuration based issues such as how to decide the initial
size of the page table and other are implementation issues such as designing a mechanism to keep pages hot while doing I/O. This issues are often
hidden in the background but must be addresses for proper paging to occur

---- Memory Notes ----
----------------------
- Process of page fault handling?: 1) hardware trap to kernel, pc saved on stack and cpu information saved,  2) ISR started to save regs and other volatile info, then call page fault handler,
3) os try to discover which virtual page needed, 4) once have virtual addr which casued fault, validate addr and look for free page frame
5) if page frame dirty, write to nonvolatile, 6) when page frame clean, bring in needed page, 7) update page table when page arrives
8) instruction backed up to state it had when it began, pc reset too, 9) process is rescheduled, 10) registers and other state information restored, return to user space




----------------------------
---- 3.7: Segementation ----
----------------------------
- having two or more seperate virtual address spaces could be better
- can provide many completely independent addr spaces: segements
- if segements ind, can grow/shrink easily
- program must supply two part address
  - segement number and addr in segment
- segements can be shared or have different protections

---- Implementation of Pure Segementation ----
----------------------------------------------
- pages are fixed size but segements are not
- allow programs and data to be broken up into logically ind addr spaces and aid sharing/protection

---- Segementation with Paging: MULTICS ----
--------------------------------------------
- only pages of segement that are actually needed have to be around
- treated each segemetn as virtual memory and page it
- segement table w/ one desciptor per segement
  - segement table itself was segemented and paged
- descriptor said if segement was in memory or not
- address has two parts: segement and address within segement
  - address within segement further divided into page number and word within page

---- Segementation with Paging: The Intel x86 ----
--------------------------------------------------
- fewer segements but larger ones
- many programs need large segements

---- Summary ----
-----------------
- You can further expand upon virtual memory by assigning each process a set of segements 
rather than splitting the existing address space up into sections. This allows each segement
to grow and shrink independendly from each other and split it up logically

---- Memory Notes ----
----------------------
- None




-------------------------
---- Chapter summary ----
-------------------------
- The simplest and earliest form of memory management is no page swapping at all. A progress is just run to completion.
You can expand on this by moving the entire process memory image onto novolatile storage and then loading in another process.
This was still not enough and hence virtual memory was designed. It provides a process with the illusion that it has nearly 
infinite contiguous memory but this is handled via pages and swapping mechanisms. A process will issue a virtual address which
will be translated by the MMU via the page table. This results in a physical address which is then used to address into memory.
All pages cannot be in memory at the same time so paging algorithms were designed to most effectivley keep pages in memory that are 
needed and replace them in the best manner. 



















































