----------------------------
| 2: Processes and threads | 
----------------------------

------------------------
---- 2.1: Processes ----
------------------------
- upon system boot, many processes are started at the same time
- in multiprogramming system cpu switches from process to process very quickly
- process model
  - all runnable software on computer organized into sequential processes
  - process: instance of an executing program
  - each process has its own virtual cpu
  - multiprogramming: switching back and forth quickly between processes
  - process is activity of some kind. it has program, input, output, and state. program is something that may be stored on disk not doing anything
  - 4 key events cause processes to be created
    - System initialization
    - Execution of process creation sys call by running process
    - User request to create new process
    - Initialization of a batch job
- daemons: processes that stay in the background to handle some activity
- fork will create an exact clone of the calling process in unix
  - have same memory image, environment strings, and same open files
  - child run execve to change its memory image and run a new program
  - parent and child PROCESS have their own distinct address space
- process terminates from one of the following conditions
  - Normal exit (voluntary): finish their work 
  - Error exit (voluntary): call exit
  - Fatal error (involuntary): discover fatal error or bug
  - Killed by another process (involuntary): kill command
- processes form hierarchy, kind of like hydra
  - init process after boot in first
- three process states
  - Running (actually using the cpu at that instant)
  - Ready (runnable, temporarily stopped to let another process run)
  - Blocked (unable to run until some other external event happens)
- state changes possible
  - running -> blocked : process blocks for input
  - running -> ready: scheduler picks another process to run
  - ready -> running: scheduler picks this process
  - blocked -> ready: input becomes available
- os maintains proces table with one entry per process
  - contains important imformation about the processes state
- each I/O class has location called interrupt vector that contains address of ISR (interrupt service routine)
  - on interrupt, computer jumps to address in interrupt vector then run ISR
- cpu utilization/degree of multiprogramming = 1 - p ** n

---- Summary ---- 
-----------------
Unix utilizes a process model in which all of the software eon the computer is organized into sequential processes. A process 
is simply an instance of an execution program. Eveything on the computer is a process. They can be both created and destoryed.
The cpu utilizes multiprocessing where multiple processes are all being executed by the cpu will switch between them quickly to 
give the illusion that they are all executing at the same time. A process can either be running, ready to run, or blocked. The 
os maintains a process table taht contaisn information about all the processes running on a system

---- Memory notes ----
----------------------
- Multiprogramming: multiple programs running simultaneously on one CPU, switches from process to process very quickly
- Process: instance of executing program. It has a program, input, output, and a state
- What is the process model: all runnable software organized into sequential processes
- 4 events that cause a process to be created: System initialization, execution of process creation sys call by running process,
user request to create new process, initialization of a batch job
- Daemons: process that stays in background to handle some activity
- Fork(): create exact clone of calling process. Same memory image, env, and open file. Parent and child process have distinct address 
spaces
- Process termination conditions: normal exit (voluntary): finished their work. Error exit (voluntary): call exit, Fatal error (involuntary): discover 
fatal error or bug. Killed by another process (involuntary): kill command
- 3 process running states: Running (actually being used by cpu at that instant), ready (runnable, temporarily stopped to let another process run),
blocked (unable to run until some other external event happens)
- How does os keep track of processes?: has process table with one entry per process, contains info about process state
- Interrupt vector: contains address of ISR (instruction service routine) for each I/O class. Upon interrupt, computer jump to addr in interrupt vector 
then runs the ISR




----------------------
---- 2.2: Threads ----
----------------------
- threads are basically mini processes
  - parallel entities share address space and data among themselves
  - lighter weight than processes, faster to create and destory
- concepts of a process
  - resource grouping
    - group together related resources such as addr space, open files, alarms, etc
  - execution
    - process is a thread of execution. has pc, registers, vars, stack, etc
- processes are used to group resources together, threads are the entities scheduled for execution on the cpu
- threads add to the process model by allowing multiple executions to take place in the same process environment
- multithreading: allowing multiple threads in the same process
  - cpu switches rapidly back and forth among threads
- threads share the same address space, but each have their own stack
- thread can be running, blocked, ready, terminated like a process
- threads can be implemented in user space or kernel space
- user space
  - kernel knows nothing about them and just mamanges single threaded process
  - run on top of run time system
  - each process needs own thread table to keep track of threads in that process, managed by run time system
  - thread switch very very fast
  - process can have customized scheduling algorithm
  - problems when implementing blocking calls and have to manually yield
  - still have to trap to kernel to handle system call
- in kerenel
  - dont need a run time system
  - kernel has single thread table that keeps track of all threads
    - does creation and deletion here
    - holds each threads registers, state, etc
  - cost of sys call to make thread is very high
- hybrid approach
  - use kernel level threads and multiplex user level threads onto some or all of them

---- Summary ----
-----------------
Within processes, there is the concept of threads. Threads can be thought of as mini processes. They are lightweight entitires
that are fast to create and destory that share sources among themselves. Processes are used to group resources together while threads
allow for multiple executions to take place in the same environemtn. Threads can be implemented in user space, the kernel, or a hybrid
approach. 

---- Memory Notes ----
---------------------
- Thread: a lighweight process that shares data/address space among themselves. Allow multiple executions to take place in the 
same environment. Have their own stack. 
- Multithreading: allowing multiple threads to execute in the same process
- User space thread impl: kernel is unaware of them and they run on top of a runtime system. Each process has a thread table that is managed by a 
runtime system that keeps tracks of all of the threads. Switching is very fast and can have customized scheduling but still have to manually yield
and trap to kernel for io
- Kernel thread impl: threads implemented in the kernel, dont need a runtime system and there is a single thread table that keeps track of all
running threads. higher overhead for thread switching and creation is expensive
- Hybrid thread impl: usage of kernel level threads and multiplex multiple user level threads onto them




-----------------------------------
---- 2.3: Event Driven Servers ----
-----------------------------------
- Threads: parallelism: blocking system calls
- Single threaded process: No parallelism, blocking system calls
- Finite-state machine/event-driven: parallelism, nonblocking system calls, interrupts

---- Summary ----
-----------------
An event driver architecture is an alternative to threads/single threaded process where there is an implementation
of a finite state machine taht responds to events and interacts with the os via nonblocking calls

---- Memory notes ----
----------------------
None




-------------------------------------------------------------
---- 2.4: Synchronization and Interprocess Communication ----
-------------------------------------------------------------
- Three problems with IPC
1) How to pass info 
2) How to make sure two or more process/threads to not get in each others way
3) Sequencing when dependencies are present
- Race conditions: when two or more processes are reading/writing to some shared data and the final result depends on who runs when
  - need to prohibit more than one process from reading/writing shared data at same time, mutual exclusion
- Critical section: part of program where shared memory is accessed
- proposals for mutual exclusion
1) Disabiling Interrupts
- on single processor system, just disable interrupts after entering critical section and re-enable after leaving
- means cpu will not switch processes
- unwise to give user process power to turn off interrupts
- if multiprocessor, only affts CPU that disabled it
- disabiling interrupts only useful in OS itself
2) Lock variables
- lock variable set to zero and when process wants to enter CS, test then set
- non atomic so still suffers race conditions
3) Strict Alternation
- keep track of whos turn it is via some variable
- busy waiting, continuously testing variable
- not good when one process is much slower
4)Peterson's Solution
- before using shred var, call enter_region with process num
- call leave_region to leave and signal that it is safe to enter
5) TSL Instruction
- test and set lock, read contents of memory word lock and store nonzero value at memory address
- atomic instruction
- alternative instr is XCHG, which x86 use
- requires busy waiting
- sleep: system call that causes caller to block/be suspended until another process wakes it up
- wakeup: takes process to be awakened and wakes it up
- producer-consumer/bounded buffer problem
  - producer puts stuff in buffer, consumer takes it out
  - producer go to sleep if buffer is full, consumer go to sleep if buffer is empty
  - race condition on the count variable that tracks amount of items in buffer, can screw up wakeups
- semaphores can be used to solve the producer consumer problem, prevents the lost wakeup
- mutex semaphore is used for mutaul exclusion
- full and empty semaphores are for synchronization
- mutex is a semaphore that does not count
  - shared variable tath can be in one of two states: locked or unlocked
  - if thread fails to get a lock, it yields
- futex: fast user space mutex
  - implements locking but avoids going into kernel unless necessary
  - consists of kernel service and user library
  - kernel service has wait queue that allow smultiple processes to wait on a lock, kernel must explicitly unblock them
- monitor: collections of prodecures, vars, and data structurs that are gropus together in special module/package
  - process may call prodecures in monitor byt cannot directly access monitors intenral data structures
  - a monitor is a language concept
  - only one processes can be in a monitor at anytime
- condition variables: signal and wait
  - if signal done on cv where several processes are waiting, only one woken up
- message passing: ipc using send and receive

---- Summary ----
-----------------
- For processes to be able to interact effectively, they need to be cooridnated and synchronize with each other to prevent invalid memory accesses
or the incorrect changing of shared memory. This is called interprocess communication. The problems of IPC are how to pass into, how to make
sure that two threads do not get in each others way, and what the sequencing is when dependencies are presents. There must be mutual exclusion
that says that only one thread can be in a critical section at the same time. Communication mechanisms for IPC include message passing, pipes, sockets
, signals, and semaphores/mutexes. 

---- Memory Notes ----
----------------------
- Three problems with IPC: how to pass information, how to make sure two or more processes/threads do not get in each others way,
sequencies when dependencies are present
- Race conditions: when two or more processes are reading/writing to some shared data and the final result depends on who runs when
- Critical section: part of program where shared memory is accessed
- Mutual exclusion - pros/cons of disabiling interrupts: pros: provides mutual exclusion, cons: not wise to give user this power, only 
affects single cpu
- Mutual exclusion - pros/cons of lock variables: variable that is set to zero and changed to one upon entering crit section. 
pros: none, cons: does not work as still suffers race conditions
- Mutual exclusion - pros/cons of Strict Alternation: keep track of whos turn it is via some variable. pros: provides mutual exclusion. 
cons: busy waiting wasts cpu cycles, not good when one process is much slower
- Mutual exclusion - TSL instruction: test and set lock, read contents of memory work lock and store nonzero value there, it is an atomic 
instr, it provides mutual exclusion, but it requires busy waiting
- producer-consumer problem: produer puts stuff on queue, consumer takes off, num items tracked by couter and producer/consumer put to sleep
when they cannot perform work. problem is when a wakeup call gets lost due to a race condition on the counter 




-------------------------
---- 2.5: Scheduling ----
-------------------------
- if computer multiprogrammed, may processes/threads computing for cpu 
- scheduler chooses which process to run next via a scheduling algorithm

---- Introduction to Scheduling ----
------------------------------------
- for most users, scheduling is simple due to a lot of waiting around and only using on process at a time
- context switch: switch from user more to kernel mode, save state of current process, restore state of other process
  - context switches are expensive, cycle wise, to preform
- processes alternative bursts of computing with bursts of I/O
- compute bound: long cpu bursts and infrequent I/O waits
- i/o bound: short cpu bursts and frequency i/o waits
- when should a scheduling decision be made?
1) when a new process is created
2) when a process exits
3) when process blocks
4) when I/o interrupt occurs
- nonpreemptive scheduling algorithm: picks process to run and lets it run until it blocks
- preemptive scheduling algorithm: picks a process and lets it run for some max time,
if still running at the end of the interval it will suspend it and pick a new process
- scheduling algorithms should be fair, enforce the policy, and keep all parts of the system busy

---- Schedling in Batch Systems ----
------------------------------------
1) first come first serve
- nonpreemptive
- processes assigned the cpu in the order they request it
- remove from front and add to back
- easy to understand and simple to implement but not good for heavy I/O
2) shortet job first
- nonpreemptive
- when runtimes are known, can just pick shortest one
- optimal only when all jobs are available simultaneously
3) Shortest remaining time next
- preemptive
- choose process whose remaining time is the shortest
- run time has to be known in advanced
- allows short jobs to get good service

---- Scheduling in Interactive Systems ----
-------------------------------------------
1) Round Robin Scheduling
- each process assigned time interval called quantum
  - process allowed to run during its quantum
- if still running at end of quantum, preempted
- if block while quantum not up, preempted
- choosing correct quantum very important
  - too short -> too many context swtich 
  - too high -> poor response to short req
  - 20-50 msec is often reasonable
2) Priority Scheduling
- each process is assigned a priority
- process with highest priority gets to run
- scheudler may decrease priority to prevent it to run idenfinitely
- could also use a time quantum
- can group processes into priority classes
  - use priority scheduling among classes
  - use round robin within class
3) Multiple queues
- setup priority classes with different quantum levels
- move down a class each time quantum expires
4) Guaranteed Schedling
- with n processes, each get 1/n cpu
- system tracks how my cpu each process has had
5) Lottery Scheduling
- give processes lottery tickets for various system resources
- upon scheduling decision, lottery ticket chosen at random and process w/ ticket gets the resource
- can give processes multiple tickets based on priority
- "all processes are equal, but some processes are more equal"
6) Fair-Share Scheduling
- some systems take into account which user owns a process before scheduling it
- each user allocated some fraction of cpu

---- Scheduling in Real Time Systems ----
-----------------------------------------
- external system generates stimuli in which device must resonse
- hard real time: absolute deadlines that must be met
- soft real time: missing occasional deadline is tolerable
- events can be periodic or aperiodic
- static scheduling makes scheduling decisions before system starts running
  - must have perfect information
- dynamic schedling makes scheduling decision at run time

---- Policy vs Mechanism ----
-----------------------------
- scheduler parameterized in some way, but params can be filled in by user process 
  - ex: syscall to change process priority

---- Thread Scheduling ----
---------------------------
- may have many processes each running many threads
  - two levels of parallelism: processes and threads
- scheduling different in user level threads or kernel level threads
1) user level threads
- kernel not aware of the threads and schedules processes like normal
- runtime makes thread scheduling decisions
- absence of clock to interrupt thread
- fast context switch
- I/O suspends the entire process
2) kernel level threads
- kernel picks a thread to run
- does not care which thread the process belongs to
- slow context switch
- I/O does not suspend the entire process

---- Summary ----
-----------------
- In a computer, there are many processs/thread competing for usage of the cpu. This is where a part of the operating system
called a scheulder comes into play. The role of the scheduler is to decide which process to run next based on a list of currently
waiting processes. There are two flavours of schedulinga algorithms. Nonpreemptive scheduling, also called cooperative, is when
a thread is picked to run and it is allowed to run until it either blocks of manually yields. Preemptive on the other hand will pick
a thread and will let it run until the os suspends it and swtiches to a new process. There are various scheduling algorithms each with
their own trade offs and seek to give all processes a fair share of the cpu, prevent starvation, and try to always keep the cpu busy.

---- Memory Notes ----
----------------------
- scheduler chooses which process to run next via some scheduling algorithm
- context switch: switch from user more to kernel mode, save state of current process, restore state of other process
- compute bound: long cpu bursts and infrequent I/O waits
- i/o bound: short cpu bursts and frequency i/o waits
- nonpreemptive scheduling algorithm: picks process to run and lets it run until it blocks
- preemptive scheduling algorithm: picks a process and lets it run for some max time,
- when should scheduling decision be made?: when new process created, when process exists, when process blocks, when I/O
interrupt occurs
- goals of a scheduling algorithm? give each process fair share of cpu, make sure policy is carried out, keep all parts of the 
system busy
- first come first serve: nonpreemptive processes assigned the cpu in the order they request it , remove from front 
and add to back, easy to understand and simple to implement but not good for heavy I/O
- shortet job first: nonpreemptive ,when runtimes are known, can just pick shortest one , optimal only when all jobs are available simultaneously
- Shortest remaining time next: preemptive, choose process whose remaining time is the shortest,run time has to be known in advanced, allows short jobs to get good service
- Round Robin Scheduling: each process assigned time interval called quantum, process allowed to run during its quantum, if still running 
at end of quantum, preempted, if block while quantum not up, preempted, choosing correct quantum very important, too short -> too many context swtich 
too high -> poor response to short req, 20-50 msec is often reasonable
- Priority Scheduling: each process is assigned a priority, process with highest priority gets to run, scheudler may decrease priority to prevent it to 
run idenfinitely, could also use a time quantum, can group processes into priority classes, use priority scheduling among classes, use round robin within class
- Multiple queues scheduling: setup priority classes with different quantum levels, move down a class each time quantum expires
- Guaranteed Schedling: with n processes, each get 1/n cpu, system tracks how my cpu each process has had
- Lottery Scheduling: give processes lottery tickets for various system resources, upon scheduling decision, lottery ticket chosen 
at random and process w/ ticket gets the resource, can give processes multiple tickets based on priority, "all processes are 
equal, but some processes are more equal"
- Fair-Share Scheduling: some systems take into account which user owns a process before scheduling it, each user allocated 
some fraction of cpu




-------------------------
---- Chapter Summary ----
-------------------------
- Operaing system provide a conceptual model of sequential processes running in parallel taht his interrupts. One key feature about processes
is that they all have their own address space independant of each other. Within processes, the os enables you to have multiple threads of control.
Threads are lighweight processes that have their own control context/information but share an address space. Threads can be implemented in user
space or kernel space. Processes must synchronize between each other to prevent concurrent access to shared data. This is done via various concurrency
primitives that prevent threads from creating deadlocks, race conditions, and various other concurrency realted bugs. Since there are multiple threads and
multiple processes running on a cpu, they have to be scheduled in a fair and consistent way. There are various different scheduling algorithms with their own
pros and cons that seek to balance the amount of cpu time each process gets on based on different factors such as process prirority
