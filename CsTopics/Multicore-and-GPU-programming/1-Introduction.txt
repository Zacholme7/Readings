-------------------
| 1: Introduction |
-------------------

--------------------------------------------
---- 1.1: The Era of multicore machines ----
--------------------------------------------
- transistors increasing exponentially, not operaing frequency
- Ghz = 1 billion cycles/second
- emergence of GPGPU computing -> general purpose computing w/ gpu
  - superiod GFlop/watt performance where energy resources are dwindling

---- Summary ----
-----------------
- Multicore machines are becoming more and more imporant as moores law is reaching its end. Modern 
cpu's are heat bound and gains in performance are being seeked out via more cores in the cpu. There 
was also the emergence of computing on a gpu that opened a new realm for high performance computing.

---- Memory Notes ----
----------------------
- How many cycles is a Ghz: 1 billion




----------------------------------------------
---- 1.2: A taxonomy of parallel machines ----
----------------------------------------------
- Michael Flynn: machine classified on how many data items they can process concurrently and
how many diff instrs they can execute at same time
  - single or multiple
- single instruction, single data (SISD): sequential machine that executes on instr at a time
- single instruction, multiple data: (SIMD): instr applied on colletion of items
- multiple instruction, single data (MISD): data can be processed by multiple machines and decision made on majority
- multiple instruction, multiple data (MIMD): most versatile cateogy, multicore machines & gpus follow this
  - subcategories
    - Shared memory MIMD: universally accessible shared memory space
      - bottleneck on scalability
      - can parition local portion for each cpu, but can still access non local portion
        - called NUMA (non uniform memory access)
      - further subcategory
        - Master-worker:some processors dedicated for execution of specialized software
        - Symmetric multiprocessing platforms: cpu idential/capable of executing any program on system
    - Distributed memory/shared nothing MIMD: process communicate by exchanging messages
      - high communication cost but able to scale

---- Summary ----
-----------------
- Michael Flynn devised a scheme to classify computing systems based on how much data and instructions
they could utilize concurrently. The division is SISD, SIMD, MISD, MIMD. You can further break MIMD into 
shared memory MIMD and distributed memory MIMD. Again, you can separate Shared memory mimd into master-worker
or Symmetric multiprocessing paltform

---- Memory Notes ----
----------------------
- SISD: sequential machine executes one instr at a time
- SIMD: single instruction multiple data, instruction applied on a collection of items
- MISD: multiple instruction single data, data process by multiple machines, decision based on majority
- MIMD: multiple instruction multiple data, most versatile, multicore & gpu
- Two categoires of MIMD: Shared memory MIMD and distributed memory/shared nothing MIMD
- Shared Memory MIMD: universally accessible shared memory space, low scalability
- Distributed memory/shared nothing MIMD: process communicate by exchanging message, high comm cost high scalability
- Two categories of shared memory MIMD: master-worker and symmetric multiprocessing platform
- Master-worker shared memory MIMD: some processors are dedicated for execution of specialized software
- Symmetric multiprocessing platforms: cpu idential/capable of executing any program on System




----------------------------------------------------------
---- 1.3: A Glimpse of influential computing machines ----
----------------------------------------------------------
- trends to either increase on chip core count or combine heterogenous cores
- gpu developed to processes massive amounts of graphics data
  - small on chip cache, lots of simple parallel ALUs, wide & fast memory buses

---- 1.3.1: The Cell BE processor ----
-------------------------------
- 2007, powered PS3
- master-worker, heterogenous, MIMD
- super hard to program but was a huge deal at the time

---- 1.3.2: Nvidias Ampere ----
------------------------
- cores arranged in groups called Streaming Multiprocessors
  - each have 64 SIMD cores
- can hit 19.5 TFlops -> 19.5 trillion floating point ops per second
- special functions called kernels
- send data to gpu -> launch a kernel -> wait to collect results
- more computation less energy

---- 1.3.3: Multicore to many-core: TILERA's TILE-Gx8072 and Intel's Xeon Phi ----
----------------------------------------------------------------------------------
- first many core from TILERA's TILE64, released August 2007
  - 64 cores originally
  - targeted networking, multimedia, cloud apps
  - co processor to offload heavy computational tasks from main cpu/host
- Xeon Phi released in 2012, stopped dev in 2017
  - lots of design advances/features developed
- often many cores means low clock frequencies

---- 1.3.4: AMD's Epyc Rome: scaling up with smaller chips ----
---------------------------------------------------------------
- cramming more compute inside gpu/gpu results in large surface area chips
  - bigger chips have more manufacturing defects
- solution was multi chip model, several smaller chips connected together rather than monolithic cpu on single chip

---- 1.3.5: Fujitsu A64FX: compute and memory integration ----
--------------------------------------------------------------
- powers most powerful machines at time of writing
- 512 bit SIMD
- 52 cores, 12 compute and one helper for each core memory group
- 4 core memory group modules

---- Summary ----
-----------------
- Processors have undergone a massive revolution over the past years. They have consistently
increased in speed and processing power and continue to do so via various groudbreaking development.

---- Memory Notes ----
----------------------
- None












































