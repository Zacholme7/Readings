--------------------------------------------
| 2: multicore and parallel program design | 
--------------------------------------------

---------------------------
---- 2.1: Introduction ----
---------------------------
- objectives are correctness and performance
- parallel execution may require a completely new algorithm

---- Summary ----
-----------------
- When moving from sequential to parallel execution, you want to maintain correctness and 
have some performance improvement. 

---- Memory Notes ----
- None




-----------------------------------
---- 2.2: The PCAM Methodology ----
-----------------------------------
- PCAM: partitioning, communication, agglomeration, and mapping
1) partitioning: breakup of computation into as many divisible pieces as possible
  - can be function (functional decomposition) or data (data decomposition) oriented
  - the number of pieces should be one to two order of magnitude bigger than the number of compute nodes
2) commuication: ideally tasks are independant, but usually some indepedencies
  - nay have to pass data between each other, the volume of data
  - results int the creation of task dependancy graph, nodes are tasks and edges are communication volume
3) agglomeration: can reduce cost of communication by grouping tasks together
  - each group assigned to single computational node, communication within group eliminated
  - nuber of groups should as one order of magnitude bigger than the number of compute nodes
4) mapping: task groups must be assigned/mapped to avaliable nodes     
  - load balane the nodes
  - reduce communcation overhead 

---- Summary ----
-----------------
- PCAM is a method for designing parallel programs. It consists of Partitioning, Communication, 
agglomeration, mapping. 

---- Memory Notes ----
----------------------
- None




-------------------------------------
---- 2.3: Decomposition patterns ----
-------------------------------------
- determining which parts of the computation can be done concurrently

                                 -> Linear    -> Task Parallelism
      -> functional decomposition|
      |                          -> Recursive -> Divide & Conquer
      |
      |                          -> Linear    -> Geometric Decomposition
start -> domain decomposition    |
      |                          -> Recursive -> Recursive Data Decomposition
      |
      |                          -> Regular   -> Pipeline 
      -> data flow decomposition |
                                 -> Irregular -> Event Based coordination

---- 2.3.1: Task Parallelism ----
---------------------------------
- break up modules and assign each to a different compute node
- use barrier to ensure synchronization
- does not scale well with the number of compute nodes

---- 2.3.2: Dividie & Conquer ----
----------------------------------
- a lot of sequential algorithms can be expressed recursively
- have base case, split into subproblems, solve each subproblem, return
- dynamic generation of tasks at runtime
- *pseudocode for generic div and conquer parallelized given on page 39*




